---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

<p>(Last Updated: May 11, 2024)</p>

<p> I am a research associate at Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) with <a href="https://nilslukas.github.io/">Dr. Nils Lukas</a>. Previously, I was working at the Singapore University of Technology and Design as a research assistant with <a href="https://info.roylee.sg/">Dr. Roy Lee</a> .</p>

<p> I recently graduated with a Bachelor in Computer Science and Engineering from <a href="https://www.iitp.ac.in/">Indian Institute of Technology Patna</a>. I was fortunate to be advised by <a href="https://www.iitp.ac.in/~sriparna/">Dr. Sriparna Saha</a> and <a href="https://www.cse.iitb.ac.in/~pb/">Prof. Pushpak Bhattacharyya</a> at <a href="https://www.iitp.ac.in/~ai-nlp-ml/">AI-NLP-ML Lab, IIT Patna</a>.  </p>

My research addresses the broader challenge of creating trustworthy machine learning systems, focusing on building models that are secure, interpretable, and aligned with safety principles. Specifically, I am interested in:

<details>
  <summary><b>Reliability</b></summary>
  Investigating the robustness of ML models against various attacks, such as adversarial attacks, backdoor attacks, and inference attacks. This includes developing effective defense mechanisms to ensure models perform reliably in diverse conditions.
</details>

<details>
  <summary><b>Interpretability</b></summary>
  Enhancing the transparency of machine learning models by understanding how features contribute to their predictions. This promotes clearer communication of model behavior, fostering trust in automated systems.
</details>

<details>
  <summary><b>Causality</b></summary>
  Exploring the cause-and-effect relationships between model inputs and outputs to enable meaningful interventions. This ensures that models are safety-aligned and behave predictably in response to changes in inputs.
</details>

<details>
  <summary><b>Misuse Prevention</b></summary>
  Examining methods to prevent malicious misuse of models, focusing on techniques such as safety alignment and watermarking to protect models from unauthorized exploitation.
</details>
